 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/app/data_loader.py b/app/data_loader.py
new file mode 100644
index 0000000..77ad052
--- /dev/null
+++ b/app/data_loader.py
@@ -0,0 +1,44 @@
+from __future__ import annotations
+
+import pandas as pd
+from functools import lru_cache
+
+from .modulobase import (
+    carregar_base_eshows,
+    carregar_base2,
+    carregar_ocorrencias,
+    carregar_base_inad,
+    carregar_pessoas,
+    carregar_npsartistas,
+)
+
+
+@lru_cache(maxsize=None)
+def get_eshows() -> pd.DataFrame:
+    return carregar_base_eshows()
+
+
+@lru_cache(maxsize=None)
+def get_base2() -> pd.DataFrame:
+    return carregar_base2()
+
+
+@lru_cache(maxsize=None)
+def get_ocorrencias() -> pd.DataFrame:
+    return carregar_ocorrencias()
+
+
+@lru_cache(maxsize=None)
+def get_inad() -> tuple[pd.DataFrame, pd.DataFrame]:
+    return carregar_base_inad()
+
+
+@lru_cache(maxsize=None)
+def get_pessoas() -> pd.DataFrame:
+    return carregar_pessoas()
+
+
+@lru_cache(maxsize=None)
+def get_npsartistas() -> pd.DataFrame:
+    return carregar_npsartistas()
+
diff --git a/app/data_manager.py b/app/data_manager.py
index 2515225..a28eca6 100644
--- a/app/data_manager.py
+++ b/app/data_manager.py
@@ -1,146 +1,185 @@
 from __future__ import annotations
 
 import datetime as _dt
 import logging
 import os
 from pathlib import Path
 from typing import Dict, Tuple
+import gc
 
 import pandas as pd
 from dotenv import find_dotenv, load_dotenv
 from postgrest import APIError
 
 from .column_mapping import rename_columns, divide_cents, CENTS_MAPPING
 
 # ────────────────────────────  logging  ────────────────────────────
 logging.basicConfig(
     level=logging.INFO,
     format="%(asctime)s – %(name)s – %(levelname)s – %(message)s",
 )
 logger = logging.getLogger("data_manager")
 logging.getLogger("httpx").setLevel(logging.WARNING)
 logging.getLogger("supabase_py").setLevel(logging.WARNING)
 
 # ───────────────────────  cache em Parquet  ────────────────────────
 CACHE_DIR = Path(__file__).resolve().parent / "_cache_parquet"
 CACHE_DIR.mkdir(exist_ok=True)
 
 CACHE_EXPIRY_HOURS: int | None = 12
 
+# Colunas essenciais a serem mantidas no cache Parquet
+COLUMNS_DASH = {
+    "baseeshows": [
+        "Id do Show", "Id da Casa", "Casa", "Cidade", "Estado", "Grupo",
+        "Nome do Artista", "Data", "Ano", "Mês", "Valor Total do Show",
+        "Valor Artista", "Comissão B2B", "Comissão B2C", "Antecipação de Cachês",
+        "Curadoria", "SaaS Percentual", "SaaS Mensalidade", "Notas Fiscais",
+        "Avaliação", "Dia do Show",
+    ],
+    "base2": [
+        "Data", "Ano", "Mês", "Faturamento", "Custos", "Imposto",
+        "LucroLiquido", "Ocupação", "Equipe", "Terceiros", "Op. Shows",
+        "D.Cliente", "Softwares", "Mkt", "D.Finan",
+    ],
+    "ocorrencias": ["DATA", "Casa", "Motivo", "Id", "Tipo"],
+    "pessoas": [
+        "Nome", "Cargo", "DataInicio", "DataFinal", "DataSaida",
+        "Salário Mensal", "AnoInicio", "MesInicio", "AnoFinal", "MesFinal",
+    ],
+    "boletocasas": ["Casa", "Valor", "Valor Real", "Status", "Data Vencimento"],
+    "boletoartistas": [
+        "NOME", "Valor Bruto", "Status", "Data Vencimento", "Adiantamento", "ID",
+    ],
+    "npsartistas": [
+        "Data", "NPS Eshows", "CSAT Eshows", "CSAT Operador 1", "CSAT Operador 2",
+    ],
+    "custosabertos": [
+        "data_competencia", "data_vencimento", "fornecedor", "valor", "Setor",
+    ],
+}
+
 def _cache_path(table: str) -> Path:
     return CACHE_DIR / f"{table.lower()}.parquet"
 
 def _is_cache_fresh(p: Path) -> bool:
     if CACHE_EXPIRY_HOURS is None or not p.exists():
         return p.exists()
     age = (_dt.datetime.now() -
            _dt.datetime.fromtimestamp(p.stat().st_mtime)).total_seconds()
     return age < CACHE_EXPIRY_HOURS * 3600
 
 def _load_parquet(table: str) -> pd.DataFrame | None:
     p = _cache_path(table)
     if _is_cache_fresh(p):
         try:
             return pd.read_parquet(p)
         except Exception:
             pass
     return None
 
 def _save_parquet(table: str, df: pd.DataFrame) -> None:
     try:
-        df.to_parquet(_cache_path(table),
-                      index=False,
-                      compression="zstd",
-                      use_dictionary=True)
+        cols = [c for c in COLUMNS_DASH.get(table.lower(), []) if c in df.columns]
+        df_to_save = df[cols] if cols else df
+        df_to_save.to_parquet(
+            _cache_path(table),
+            index=False,
+            compression="zstd",
+            use_dictionary=True,
+        )
     except Exception:
         pass
 
 # ────────────────────────  Supabase client  ────────────────────────
 supa = None  # lazily-instantiated singleton
 
 def _init_supabase():
     global supa
     if supa is not None:
         return supa
 
     if (env := find_dotenv()):
         load_dotenv(env)
 
     url, key = os.getenv("SUPABASE_URL", ""), os.getenv("SUPABASE_KEY", "")
     if not (url and key):
         logger.error("SUPABASE_URL/KEY não encontrados.")
         return None
 
     try:
         from supabase import create_client
         supa = create_client(url, key)
         logger.info("Cliente Supabase inicializado.")
     except Exception as e:
         logger.error("Falha ao criar cliente Supabase: %s", e)
 
     return supa
 
 supa = _init_supabase()
 
 # ─────────────────────────  helpers comuns  ────────────────────────
 _cache: Dict[str, pd.DataFrame] = {}
 
 def dedup(df: pd.DataFrame) -> pd.DataFrame:
     if not df.empty and df.columns.duplicated().any():
         df = df.loc[:, ~df.columns.duplicated(keep="first")]
     return df
 
 # ────────────────────────  download + limpeza  ─────────────────────
 def _fetch(table: str) -> pd.DataFrame:
     if supa is None:
         return pd.DataFrame()
 
-    STEP, page, pages = 1000, 0, []
+    STEP, page = 1000, 0
+    result = pd.DataFrame()
     while True:
         start, end = page * STEP, (page + 1) * STEP - 1
         try:
             q = supa.table(table).select("*")
             if table.lower() == "baseeshows":
                 q = q.gte("Data", "2022-01-01")
             resp = q.range(start, end).execute()
         except APIError as err:
             logger.error("[%s] página %s: %s", table, page + 1, err.message)
             break
 
         data = resp.data or []
         if not data:
             break
-        pages.append(pd.DataFrame(data))
+        page_df = pd.DataFrame(data)
+        result = pd.concat([result, page_df], ignore_index=True, copy=False)
+        gc.collect()
         if len(data) < STEP:
             break
         page += 1
 
-    if not pages:
+    if result.empty:
         return pd.DataFrame()
 
-    df = pd.concat(pages, ignore_index=True)
+    df = result
     df = divide_cents(dedup(rename_columns(df, table)), table)
 
     for col in CENTS_MAPPING.get(table.lower(), []):
         if col in df.columns:
             df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)
 
     logger.info("[%s] baixado: %s linhas × %s col", table, *df.shape)
     return df
 
 # ───────────────────────  cache RAM + Parquet  ─────────────────────
 def _get(table: str, *, force_reload: bool = False) -> pd.DataFrame:
     table = table.lower()
     logger.info("carregando %s…", table)
 
     if not force_reload and table in _cache:
         return _cache[table]
 
     if not force_reload:
         if (df_disk := _load_parquet(table)) is not None:
             _cache[table] = df_disk
             logger.info("[%s] carregado do Parquet (%s linhas)", table, len(df_disk))
             return df_disk
 
     df_live = _fetch(table)
     _cache[table] = df_live
diff --git a/app/main.py b/app/main.py
index 632959b..ac719b2 100644
--- a/app/main.py
+++ b/app/main.py
@@ -20,50 +20,55 @@ from datetime import datetime, timedelta
 import calendar
 
 # ― Terceiros -------------------------------------------------------------------
 import warnings
 
 import pandas as pd
 import numpy as np
 import dash
 import dash_bootstrap_components as dbc
 from dash import Dash, html, dcc, Input, Output, State, ALL, callback_context
 from dash.exceptions import PreventUpdate
 import plotly.express as px
 import plotly.graph_objects as go
 from dateutil.relativedelta import relativedelta
 import colorsys  # usado em funções de contraste e gráficos
 from dash import callback
 
 # ― Módulos internos ------------------------------------------------------------
 from app.config_data import HIST_KPI_MAP, get_hist_kpi_map
 from .modulobase import (
     carregar_base_eshows,
     carregar_eshows_excluidos,  # para exportar registros descartados
     carregar_base2,
     carregar_ocorrencias,
 )
+from .data_loader import (
+    get_eshows,
+    get_base2,
+    get_ocorrencias,
+)
 from .utils import (
     formatar_range_legivel,
     formatar_valor_utils,
     floatify_hist_data,
     filtrar_periodo_principal,
     filtrar_periodo_comparacao,
     get_period_start,
     get_period_end,
     mes_nome,
     mes_nome_intervalo,
     calcular_periodo_anterior,
     filtrar_base2,
     filtrar_base2_comparacao,
     filtrar_base2_op_shows,
     filtrar_base2_op_shows_compare,
     faturamento_dos_grupos,
     obter_top5_grupos_ano_anterior,
     novos_palcos_dos_grupos,
     get_churn_ka_for_period,
     get_period_range,
     calcular_churn,
     filtrar_novos_palcos_por_comparacao,
     filtrar_novos_palcos_por_periodo,
     calcular_churn_novos_palcos,
     calcular_variacao_percentual,
@@ -319,53 +324,53 @@ def _add_cost_donut(fig, categories, values, labels, vals):
         yref="y domain",
         ax=x1,
         ay=y0,
         axref="x domain",
         ayref="y domain",
         text="",
         showarrow=True,
         arrowhead=1,
         arrowsize=1,
         arrowwidth=2,
         arrowcolor="#F57D7C",
     )
 
 # ==============================================================================
 # 5) ASSETS E ESTILOS EXTERNOS
 # ==============================================================================
 external_stylesheets = [
     dbc.themes.BOOTSTRAP,
     "https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css",
 ]
 
 # ==============================================================================
 # 6) CARREGAMENTO DAS BASES PRINCIPAIS
 # ==============================================================================
 logger.info("Carregando bases do Supabase…")
-df_eshows = carregar_base_eshows()
-df_base2 = carregar_base2()
-df_ocorrencias = carregar_ocorrencias()
+df_eshows = get_eshows()
+df_base2 = get_base2()
+df_ocorrencias = get_ocorrencias()
 logger.info("Bases carregadas com sucesso.")
 
 # ― Ajustes auxiliares para Novos Clientes -------------------------------------
 if df_eshows is not None and not df_eshows.empty:
     df_casas_earliest = (
         df_eshows.groupby("Id da Casa")["Data do Show"].min().reset_index(name="EarliestShow")
     )
     df_casas_latest = (
         df_eshows.groupby("Id da Casa")["Data do Show"].max().reset_index(name="LastShow")
     )
 else:
     df_casas_earliest = df_casas_latest = None
 
 # ==============================================================================
 # 7) UTILITÁRIO DE ESTADOS (UF → Nome/Bandeira)
 # ==============================================================================
 with open(Path(__file__).resolve().parent / "data" / "uf.json", "r", encoding="utf-8") as fp:
     data_uf = json.load(fp)
 
 SIGLA_TO_NOME = {uf["sigla"]: uf["nome"] for uf in data_uf["UF"]}
 
 
 def get_nome_estado(sigla: str) -> str:
     """Converte sigla (SP) em nome do estado (São Paulo)."""
     return SIGLA_TO_NOME.get(sigla, sigla)
diff --git a/app/modulobase.py b/app/modulobase.py
index b0c1771..d4cba8e 100644
--- a/app/modulobase.py
+++ b/app/modulobase.py
@@ -41,65 +41,63 @@ logger = logging.getLogger("modulobase")
 if not logger.handlers:
     logging.basicConfig(level=logging.INFO)
 
 # ────────────────────────────  caches  ──────────────────────────────
 _df_eshows_cache:            pd.DataFrame | None = None
 _df_eshows_excluidos_cache:  pd.DataFrame | None = None
 _df_base2_cache:             pd.DataFrame | None = None
 _df_pessoas_cache:           pd.DataFrame | None = None
 _df_ocorrencias_cache:       pd.DataFrame | None = None
 _inad_casas_cache:           pd.DataFrame | None = None
 _inad_artistas_cache:        pd.DataFrame | None = None
 _df_metas_cache:             pd.DataFrame | None = None
 _df_custosabertos_cache:     pd.DataFrame | None = None          # NOVO
 _df_npsartistas_cache:       pd.DataFrame | None = None          # NOVO
 
 # ╭───────────────────────────  helpers  ─────────────────────────────╮
 def _slug(text: str) -> str:
     text = unicodedata.normalize("NFD", str(text))
     return re.sub(r"[^0-9a-zA-Z]+", "_", text).strip("_").lower()
 
 
 def otimizar_tipos(df: pd.DataFrame) -> pd.DataFrame:
     if df.empty:
         return df
 
-    df2 = df.copy()
+    for col in df.select_dtypes(include="int64").columns:
+        df[col] = pd.to_numeric(df[col], downcast="integer", errors="ignore")
+    for col in df.select_dtypes(include="float64").columns:
+        df[col] = pd.to_numeric(df[col], downcast="float", errors="ignore")
 
-    for col in df2.select_dtypes(include="int64").columns:
-        df2[col] = pd.to_numeric(df2[col], downcast="integer")
-    for col in df2.select_dtypes(include="float64").columns:
-        df2[col] = pd.to_numeric(df2[col], downcast="float")
-
-    for col in df2.select_dtypes(include="object").columns:
+    for col in df.select_dtypes(include="object").columns:
         try:
-            nunq = df2[col].nunique(dropna=False)
-            if nunq and nunq / len(df2) < 0.5:
-                df2[col] = df2[col].astype("category")
+            nunq = df[col].nunique(dropna=False)
+            if nunq and nunq / len(df) < 0.5:
+                df[col] = df[col].astype("category")
         except TypeError:
             pass
-    return df2
+    return df
 
 # ──────────────────  SANITIZE NPS Artistas  ───────────────────────
 def sanitize_npsartistas_df(df_raw: pd.DataFrame) -> pd.DataFrame:
     """
     • Converte Data → datetime
     • Garante tipos numéricos inteiros nas colunas NPS/CSAT
     """
     df = rename_columns(dedup(df_raw), "npsartistas").copy()
 
     if "Data" in df.columns:
         df["Data"] = pd.to_datetime(df["Data"], errors="coerce")
 
     for col in ("NPS Eshows", "CSAT Eshows", "CSAT Operador 1", "CSAT Operador 2"):
         if col in df.columns:
             df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int16", errors="ignore")
 
     return otimizar_tipos(df.reset_index(drop=True))
 
 # ╭──────────────────  SANITIZE Custos Abertos  ──────────────────────╮
 def sanitize_custosabertos_df(df_raw: pd.DataFrame) -> pd.DataFrame:
     """
     • Converte data_competencia / data_vencimento para datetime
     • Adiciona coluna Setor via SUPPLIER_TO_SETOR
     • Valor já em centavos (int64)
     """
diff --git a/app/utils.py b/app/utils.py
index 4a2adc4..353dfbe 100644
--- a/app/utils.py
+++ b/app/utils.py
@@ -1,65 +1,64 @@
 import pandas as pd
 import json
 import os
 import numbers
 from datetime import datetime, timedelta
 import calendar
 import pandas as pd
 import calendar
 from datetime import datetime
 import sys
 from dateutil.relativedelta import relativedelta
 
 # =================================================================================
 # IMPORTAÇÃO DE FUNÇÕES DE CARREGAMENTO (modulobase) E FORMATAÇÃO (utils)
 # =================================================================================
 from .modulobase import (
-    carregar_base_eshows,
     carregar_eshows_excluidos,  # p/ exportar as linhas excluídas
+    carregar_base_eshows,
     carregar_base2,
     carregar_ocorrencias,
     carregar_base_inad,
     carregar_pessoas,
-    carregar_npsartistas
+    carregar_npsartistas,
+)
+from .data_loader import (
+    get_eshows,
+    get_base2,
+    get_ocorrencias,
+    get_inad,
+    get_pessoas,
+    get_npsartistas,
 )
 
 # =================================================================================
 # CARREGAMENTO DAS BASES GERAIS (removido uso de variáveis globais duplicadas)
 # =================================================================================
 # Utilize carregar_base_eshows(), carregar_base2(), carregar_ocorrencias() etc. sempre que precisar dos dados.
 
-# Load global DataFrames for util functions
-try:
-    df_eshows = carregar_base_eshows()
-    df_base2 = carregar_base2()
-    df_ocorrencias = carregar_ocorrencias()
-    df_inad = carregar_base_inad()
-    df_pessoas = carregar_pessoas()
-    df_npsartistas = carregar_npsartistas()
-except Exception:
-    df_eshows = df_base2 = df_ocorrencias = df_inad = df_pessoas = df_npsartistas = None
+
 
 def ensure_grupo_col(df):
     """
     Garante que o DataFrame tenha a coluna 'Grupo'.
     · Se já existe, devolve inalterado.
     · Se encontrar outra coluna cujo nome contenha 'grupo', renomeia para 'Grupo'.
     · Caso contrário, cria a coluna vazia.
     """
     if df is None or df.empty:
         return df
     if 'Grupo' in df.columns:
         return df
     possiveis = [c for c in df.columns if 'grupo' in c.lower()]
     if possiveis:
         df = df.rename(columns={possiveis[0]: 'Grupo'})
     else:
         df['Grupo'] = None
     return df
 
 # ===============================
 # Mapeamento de Bases dos KPIs
 # ===============================
 kpi_bases_mapping = {
     "CMGR": ["eshows"],
     "Lucratividade": ["eshows", "base2"],
@@ -993,50 +992,51 @@ def calcular_churn(ano, periodo, mes, start_date=None, end_date=None, uf=None, d
     Se a casa não fez show após LastShow, e data_churn cai no período, conta churn.
     
     Args:
         ano (int): Ano de referência
         periodo (str): Período selecionado
         mes (int, optional): Mês específico (caso período seja 'Mês Aberto')
         start_date (datetime, optional): Data inicial
         end_date (datetime, optional): Data final
         uf (str, optional): Estado para filtrar (ex: 'SP')
         dias_sem_show (int): Dias sem show para considerar churn
         custom_range (tuple, optional): (data_inicial, data_final) para período personalizado
     
     Returns:
         int: Quantidade de casas que deram churn no período
     """
     # Ajusta o custom_range ou date_range para usar em get_period_start/end
     date_range = None
     if custom_range is not None:
         date_range = custom_range
     elif start_date is not None and end_date is not None:
         date_range = (start_date, end_date)
     
     start_periodo = get_period_start(ano, periodo, mes, date_range)
     end_periodo = get_period_end(ano, periodo, mes, date_range)
 
+    df_eshows = get_eshows()
     if df_eshows is None or df_eshows.empty:
         return 0
 
     df_relevante = df_eshows
     if uf and uf != "BR":
         df_relevante = df_relevante[df_relevante["Estado"] == uf]
     if df_relevante.empty:
         return 0
 
     df_last = (
         df_relevante
         .groupby("Id da Casa")["Data do Show"]
         .max()
         .reset_index(name="LastShow")
     )
 
     df_last["data_churn"] = df_last["LastShow"] + timedelta(days=dias_sem_show)
 
     df_churn_cand = df_last[df_last["data_churn"] <= end_periodo].copy()
     if df_churn_cand.empty:
         return 0
 
     casas_cand = df_churn_cand["Id da Casa"].unique()
     df_shows_cand = df_relevante[df_relevante["Id da Casa"].isin(casas_cand)]
     df_churn_cand = pd.merge(
 
EOF
)